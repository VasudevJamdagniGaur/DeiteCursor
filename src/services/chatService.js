class ChatService {
  constructor() {
    this.baseURL = 'https://akl4kcr0r58foi-11434.proxy.runpod.net/';
  }

  async sendMessage(userMessage, conversationHistory = [], onToken = null) {
    console.log('üöÄ CHAT DEBUG: Starting sendMessage with:', userMessage);
    console.log('üöÄ CHAT DEBUG: Using URL:', this.baseURL);
    console.log('üöÄ CHAT DEBUG: Streaming enabled:', !!onToken);
    
    try {
      // Prepare messages for the API
      const messages = [
        {
          role: 'system',
          content: 'You are Deite, a warm and emotionally intelligent AI companion. Respond naturally and empathetically to the user.'
        },
        // Add conversation history
        ...conversationHistory.slice(-5).map(msg => ({
          role: msg.sender === 'user' ? 'user' : 'assistant',
          content: msg.text
        })),
        {
          role: 'user',
          content: userMessage
        }
      ];

      console.log('üì§ CHAT DEBUG: Prepared messages:', messages);

      // First, let's check what models are available
      let availableModels = [];
      try {
        const modelsResponse = await fetch(`${this.baseURL}api/tags`, {
          method: 'GET',
          headers: { 'Content-Type': 'application/json' }
        });
        if (modelsResponse.ok) {
          const modelsData = await modelsResponse.json();
          availableModels = modelsData.models?.map(m => m.name) || [];
          console.log('üîç CHAT DEBUG: Available models:', availableModels);
        }
      } catch (e) {
        console.log('üîç CHAT DEBUG: Could not fetch models:', e.message);
      }

      // Create a simple prompt from messages for generate API
      const simplePrompt = messages.map(msg => {
        if (msg.role === 'system') return msg.content;
        if (msg.role === 'user') return `Human: ${msg.content}`;
        if (msg.role === 'assistant') return `Assistant: ${msg.content}`;
        return msg.content;
      }).join('\n\n') + '\n\nAssistant:';

      // Try multiple approaches to make this work
      const attempts = [
        // Attempt 1: Basic connection test first
        {
          url: `${this.baseURL}`,
          method: 'GET',
          name: 'Basic Connection Test'
        },
        // Attempt 2: Check available models
        {
          url: `${this.baseURL}api/tags`,
          method: 'GET',
          name: 'Get Available Models'
        },
        // Attempt 3: Generate API with common models (streaming enabled)
        {
          url: `${this.baseURL}api/generate`,
          body: {
            model: 'llama3.1',
            prompt: simplePrompt,
            stream: true  // üëà ALWAYS enable streaming
          },
          name: 'Ollama Generate API (llama3.1)',
          streaming: true
        },
        {
          url: `${this.baseURL}api/generate`,
          body: {
            model: 'llama3',
            prompt: simplePrompt,
            stream: true  // üëà ALWAYS enable streaming
          },
          name: 'Ollama Generate API (llama3)',
          streaming: true
        },
        {
          url: `${this.baseURL}api/generate`,
          body: {
            model: 'llama2',
            prompt: simplePrompt,
            stream: true  // üëà ALWAYS enable streaming
          },
          name: 'Ollama Generate API (llama2)',
          streaming: true
        },
        // Attempt 4: Try with first available model if we found any
        ...(availableModels.length > 0 ? [{
          url: `${this.baseURL}api/generate`,
          body: {
            model: availableModels[0],
            prompt: simplePrompt,
            stream: true  // üëà ALWAYS enable streaming
          },
          name: `Ollama Generate API (${availableModels[0]})`,
          streaming: true
        }] : []),
        // Attempt 5: Chat API attempts (streaming enabled)
        {
          url: `${this.baseURL}api/chat`,
          body: {
            model: 'llama3.1',
            messages: messages,
            stream: true  // üëà ALWAYS enable streaming
          },
          name: 'Ollama Chat API (llama3.1)',
          streaming: true
        },
        {
          url: `${this.baseURL}api/chat`,
          body: {
            model: 'llama3',
            messages: messages,
            stream: true  // üëà ALWAYS enable streaming
          },
          name: 'Ollama Chat API (llama3)',
          streaming: true
        },
        // Attempt 6: Try OpenAI compatible format (streaming enabled)
        {
          url: `${this.baseURL}v1/chat/completions`,
          body: {
            model: 'llama3.1',
            messages: messages,
            stream: true,  // üëà Enable streaming for OpenAI format too
            max_tokens: 1000
          },
          name: 'OpenAI Compatible API',
          streaming: true
        }
      ];

      for (const attempt of attempts) {
        try {
          console.log(`üîÑ CHAT DEBUG: Trying ${attempt.name}...`);
          console.log(`üîÑ CHAT DEBUG: URL: ${attempt.url}`);
          console.log(`üîÑ CHAT DEBUG: Body:`, attempt.body);
          
          const fetchOptions = {
            method: attempt.method || 'POST',
            headers: attempt.headers || {
              'Content-Type': 'application/json',
              'Accept': 'application/json',
            }
          };
          
          if (attempt.body && fetchOptions.method === 'POST') {
            fetchOptions.body = JSON.stringify(attempt.body);
          }
          
          const response = await fetch(attempt.url, fetchOptions);
          
          console.log(`üì• CHAT DEBUG: ${attempt.name} Response status:`, response.status);
          console.log(`üì• CHAT DEBUG: ${attempt.name} Response headers:`, [...response.headers.entries()]);
          
          if (response.ok) {
            // Handle GET requests differently (connection tests)
            if (attempt.method === 'GET') {
              const responseText = await response.text();
              console.log(`‚úÖ CHAT DEBUG: ${attempt.name} Response:`, responseText);
              
              // For GET requests, just continue to next attempt unless it's a model list
              if (attempt.name === 'Get Available Models') {
                try {
                  const data = JSON.parse(responseText);
                  console.log('üìã CHAT DEBUG: Models data:', data);
                } catch (e) {
                  console.log('üìã CHAT DEBUG: Could not parse models response');
                }
              }
              continue; // Don't return for GET requests, just log and continue
            }
            
            // Handle streaming responses
            if (attempt.streaming && onToken) {
              console.log('üåä CHAT DEBUG: Processing streaming response...');
              return await this.handleStreamingResponse(response, onToken, attempt.name);
            }
            
            // Handle non-streaming POST requests (actual chat attempts)
            const data = await response.json();
            console.log(`‚úÖ CHAT DEBUG: ${attempt.name} Response data:`, data);
            
            // Handle different response formats
            if (data.message && data.message.content) {
              console.log('‚úÖ CHAT DEBUG: Using message.content format');
              return data.message.content.trim();
            } else if (data.response) {
              console.log('‚úÖ CHAT DEBUG: Using response format');
              return data.response.trim();
            } else if (data.content) {
              console.log('‚úÖ CHAT DEBUG: Using content format');
              return data.content.trim();
            } else if (data.choices && data.choices[0] && data.choices[0].message) {
              console.log('‚úÖ CHAT DEBUG: Using OpenAI format');
              return data.choices[0].message.content.trim();
            } else if (typeof data === 'string') {
              console.log('‚úÖ CHAT DEBUG: Using string format');
              return data.trim();
            }
            
            console.log('‚ö†Ô∏è CHAT DEBUG: Unknown response format, trying to extract text...');
            const responseText = JSON.stringify(data);
            if (responseText.length > 50) {
              return `Response received but format unknown: ${responseText.substring(0, 200)}...`;
            }
          } else {
            const errorText = await response.text();
            console.log(`‚ùå CHAT DEBUG: ${attempt.name} Error response (${response.status}):`, errorText);
          }
        } catch (attemptError) {
          console.log(`‚ùå CHAT DEBUG: ${attempt.name} failed:`, attemptError.message);
        }
      }

      // If all attempts fail, return a debug response so we can at least see the chat is working
      console.log('‚ö†Ô∏è CHAT DEBUG: All API attempts failed, returning debug response');
      return `I'm having trouble connecting to the AI service right now, but I can see your message: "${userMessage}". Please check the browser console for detailed debug information. The system is trying to connect to: ${this.baseURL}`;

    } catch (error) {
      console.error('‚ùå CHAT DEBUG: Final error:', error);
      // Return error info instead of throwing
      return `Debug Error: ${error.message}. Check console for details.`;
    }
  }

  async handleStreamingResponse(response, onToken, attemptName) {
    console.log('üåä STREAM DEBUG: Starting to process streaming response...');
    
    try {
      const reader = response.body.getReader();
      const decoder = new TextDecoder();
      let fullResponse = '';
      
      while (true) {
        const { done, value } = await reader.read();
        
        if (done) {
          console.log('üåä STREAM DEBUG: Stream completed');
          break;
        }
        
        const chunk = decoder.decode(value, { stream: true });
        console.log('üåä STREAM DEBUG: IMMEDIATE CHUNK processing:', chunk);
        
        // Process lines immediately as they arrive - no waiting!
        const lines = chunk.split('\n').filter(line => line.trim());
        
        for (const line of lines) {
          try {
            // Try to parse as JSON (Ollama format)
            const data = JSON.parse(line);
            
            // Handle Ollama generate API streaming format - IMMEDIATE processing
            if (data.response !== undefined) {
              const token = data.response;
              console.log('üåä STREAM DEBUG: IMMEDIATE TOKEN from response field:', token);
              fullResponse += token;
              // Send token IMMEDIATELY to UI - no delays!
              if (onToken && token) {
                onToken(token);
              }
            }
            // Handle Ollama chat API streaming format - IMMEDIATE processing
            else if (data.message && data.message.content !== undefined) {
              const token = data.message.content;
              console.log('üåä STREAM DEBUG: IMMEDIATE TOKEN from message.content:', token);
              fullResponse += token;
              // Send token IMMEDIATELY to UI - no delays!
              if (onToken && token) {
                onToken(token);
              }
            }
            // Handle OpenAI streaming format - IMMEDIATE processing
            else if (data.choices && data.choices[0] && data.choices[0].delta && data.choices[0].delta.content) {
              const token = data.choices[0].delta.content;
              console.log('üåä STREAM DEBUG: IMMEDIATE TOKEN from OpenAI delta:', token);
              fullResponse += token;
              // Send token IMMEDIATELY to UI - no delays!
              if (onToken && token) {
                onToken(token);
              }
            }
            
            // Check if stream is done
            if (data.done === true) {
              console.log('üåä STREAM DEBUG: Stream marked as done');
              break;
            }
            
          } catch (parseError) {
            // If not JSON, might be raw text
            console.log('üåä STREAM DEBUG: Non-JSON chunk, treating as raw text:', line);
            if (line.trim()) {
              fullResponse += line;
              if (onToken) onToken(line);
            }
          }
        }
      }
      
      console.log('üåä STREAM DEBUG: Final response:', fullResponse);
      return fullResponse.trim();
      
    } catch (streamError) {
      console.error('‚ùå STREAM DEBUG: Error processing stream:', streamError);
      throw streamError;
    }
  }

  async testConnection() {
    console.log('üîç Testing RunPod connection...');
    
    try {
      // First, try to check if Ollama is running
      const healthResponse = await fetch(`${this.baseURL}`, {
        method: 'GET',
        headers: {
          'Content-Type': 'application/json',
        }
      });

      console.log('Health check response:', healthResponse.status);

      if (healthResponse.ok) {
        // Try to get available models
        try {
          const modelsResponse = await fetch(`${this.baseURL}/api/tags`, {
            method: 'GET',
            headers: {
              'Content-Type': 'application/json',
            }
          });

          if (modelsResponse.ok) {
            const modelsData = await modelsResponse.json();
            console.log('Available models:', modelsData);
            
            return {
              status: 'success',
              message: 'RunPod connection successful',
              details: `Available models: ${modelsData.models?.map(m => m.name).join(', ') || 'No models found'}`
            };
          }
        } catch (modelsError) {
          console.log('Models endpoint not available, but base connection works');
        }

        return {
          status: 'success',
          message: 'RunPod connection successful',
          details: 'Ollama is running'
        };
      }

      return {
        status: 'error',
        message: 'RunPod connection failed',
        details: `HTTP ${healthResponse.status}`
      };

    } catch (error) {
      console.error('Connection test error:', error);
      return {
        status: 'error',
        message: 'RunPod connection failed',
        details: error.message
      };
    }
  }

  async generateDayDescription(emotionalData, dayType, period) {
    console.log(`ü§ñ Generating ${dayType} day description for ${period}...`);
    
    try {
      const { happiness, energy, stress, anxiety, timestamp } = emotionalData;
      const date = new Date(timestamp).toLocaleDateString();
      
      let prompt;
      if (dayType === 'best') {
        prompt = `On ${date}, this person had their best mood day in the ${period} with happiness at ${happiness}%, energy at ${energy}%, stress at ${stress}%, and anxiety at ${anxiety}%. Create a mini-story explaining what made this day great. Write 2-3 sentences that tell what likely happened and how it made them feel throughout the day.

Format like this example:
"You felt upbeat and motivated because you finally completed an important task you'd been putting off. High energy and happiness carried through the day, making even small activities feel rewarding."

Focus on:
- A specific achievement, breakthrough, or positive event
- How that led to sustained good feelings
- The ripple effect on the rest of the day

Write a realistic scenario based on the emotion levels.`;
      } else {
        prompt = `On ${date}, this person had their most challenging day in the ${period} with happiness at ${happiness}%, energy at ${energy}%, stress at ${stress}%, and anxiety at ${anxiety}%. Create a mini-story explaining what made this day difficult. Write 2-3 sentences that tell what likely happened and how it affected them throughout the day.

Format like this example:
"The day felt tough as stress from upcoming deadlines combined with anxious thoughts about the outcome. These worries overshadowed your mood, making it harder to stay positive and focused."

Focus on:
- A specific stressor, challenge, or difficult situation
- How that created ongoing emotional impact
- The effect on their overall mood and day

Write a realistic scenario based on the emotion levels.`;
      }

      const messages = [
        {
          role: 'system',
          content: 'You are an empathetic emotional wellness assistant. Create brief, story-like explanations that connect specific events to emotional patterns. Write 2-3 sentences that tell what likely happened and how it affected their day. Be realistic and personal, avoiding generic emotion descriptions.'
        },
        {
          role: 'user',
          content: prompt
        }
      ];

      // Convert messages to a single prompt for Ollama generate API
      const promptText = messages.map(msg => {
        if (msg.role === 'system') return msg.content;
        if (msg.role === 'user') return `User: ${msg.content}`;
        return msg.content;
      }).join('\n\n');

      console.log('üöÄ Calling RunPod with prompt:', promptText.substring(0, 200) + '...');

      const response = await fetch(`${this.baseURL}api/generate`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: 'llama3:70b',
          prompt: promptText,
          stream: false,
          options: {
            temperature: 0.7,
            max_tokens: 200
          }
        })
      });

      if (!response.ok) {
        throw new Error(`API error: ${response.status}`);
      }

      const data = await response.json();
      console.log('‚úÖ RunPod response:', data);
      
      if (data.response) {
        return data.response.trim();
      }

      throw new Error('Invalid response format');

    } catch (error) {
      console.error(`‚ùå Error generating ${dayType} day description:`, error);
      // Return fallback mini-story descriptions
      if (dayType === 'best') {
        return "You felt upbeat and motivated because you accomplished meaningful tasks that brought you satisfaction. High energy and happiness carried through the day, making even routine activities feel rewarding and enjoyable.";
      } else {
        return "The day felt challenging as stress from various responsibilities combined with some anxious thoughts about the future. These pressures made it harder to maintain your usual positive outlook and focus throughout the day.";
      }
    }
  }

  async generateComprehensiveAnalysis(emotionalData, period) {
    console.log(`ü§ñ Generating comprehensive AI analysis for ${period} period...`);
    
    try {
      // Prepare emotional data summary
      const dataLength = emotionalData.length;
      if (dataLength === 0) {
        throw new Error('No emotional data available for analysis');
      }

      // Calculate averages
      const avgHappiness = emotionalData.reduce((sum, day) => sum + day.happiness, 0) / dataLength;
      const avgEnergy = emotionalData.reduce((sum, day) => sum + day.energy, 0) / dataLength;
      const avgAnxiety = emotionalData.reduce((sum, day) => sum + day.anxiety, 0) / dataLength;
      const avgStress = emotionalData.reduce((sum, day) => sum + day.stress, 0) / dataLength;

      // Find extremes
      const bestDay = emotionalData.reduce((best, current) => 
        (current.happiness + current.energy) > (best.happiness + best.energy) ? current : best
      );
      const worstDay = emotionalData.reduce((worst, current) => 
        (current.anxiety + current.stress) > (worst.anxiety + worst.stress) ? current : worst
      );

      const prompt = `As an AI emotional wellness analyst, analyze this person's emotional data over ${period} and provide insights in JSON format.

**Emotional Data Summary:**
- Period: ${period}
- Days tracked: ${dataLength}
- Average Happiness: ${Math.round(avgHappiness)}%
- Average Energy: ${Math.round(avgEnergy)}%
- Average Anxiety: ${Math.round(avgAnxiety)}%
- Average Stress: ${Math.round(avgStress)}%
- Best day: ${new Date(bestDay.timestamp).toLocaleDateString()} (H:${bestDay.happiness}%, E:${bestDay.energy}%)
- Most challenging day: ${new Date(worstDay.timestamp).toLocaleDateString()} (A:${worstDay.anxiety}%, S:${worstDay.stress}%)

Provide analysis in this exact JSON format:
{
  "highlights": {
    "bestDayReason": "Brief explanation why this was the best day based on emotional scores",
    "challengingDayReason": "Brief explanation why this was the most challenging day"
  },
  "triggers": {
    "stressFactors": ["factor1", "factor2", "factor3"],
    "joyFactors": ["factor1", "factor2", "factor3"],
    "energyDrains": ["factor1", "factor2", "factor3"]
  },
  "patterns": {
    "overallTrend": "improving/stable/declining",
    "keyInsight": "Main pattern observed in the data",
    "recommendation": "Specific actionable advice"
  },
  "emotionalBalance": {
    "dominantEmotion": "happiness/energy/anxiety/stress",
    "balanceScore": 75,
    "insight": "Brief insight about emotional balance"
  },
  "personalizedGuidance": {
    "focus": "What to focus on improving",
    "strength": "What they're doing well",
    "actionStep": "One specific action to take"
  }
}`;

      const messages = [
        {
          role: 'system',
          content: 'You are an expert emotional wellness analyst. Provide detailed, empathetic, and actionable insights based on emotional data. Always respond with valid JSON only.'
        },
        {
          role: 'user',
          content: prompt
        }
      ];

      const response = await fetch(`${this.baseURL}/api/chat`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: 'llama3.1:70b',
          messages: messages,
          stream: false,
          options: {
            temperature: 0.7,
            top_p: 0.9
          }
        })
      });

      if (!response.ok) {
        throw new Error(`API error: ${response.status}`);
      }

      const data = await response.json();
      
      if (data.message && data.message.content) {
        const responseText = data.message.content.trim();
        
        // Parse JSON response
        const jsonMatch = responseText.match(/\{[\s\S]*\}/);
        if (jsonMatch) {
          const analysis = JSON.parse(jsonMatch[0]);
          return analysis;
        }
        
        throw new Error('Invalid JSON format in response');
      }

      throw new Error('Invalid response format');

    } catch (error) {
      console.error(`‚ùå Error generating comprehensive analysis:`, error);
      
      // Return fallback analysis
      return {
        highlights: {
          bestDayReason: "High energy and happiness levels created an optimal emotional state.",
          challengingDayReason: "Elevated stress and anxiety levels made this day more difficult."
        },
        triggers: {
          stressFactors: ["Work pressure", "Time constraints", "Uncertainty"],
          joyFactors: ["Meaningful conversations", "Personal achievements", "Relaxation"],
          energyDrains: ["Overthinking", "Worry cycles", "Fatigue"]
        },
        patterns: {
          overallTrend: "stable",
          keyInsight: "Your emotional patterns show normal daily variations.",
          recommendation: "Continue monitoring and practicing self-care."
        },
        emotionalBalance: {
          dominantEmotion: "happiness",
          balanceScore: 70,
          insight: "Generally maintaining good emotional balance with room for improvement."
        },
        personalizedGuidance: {
          focus: "Stress management and energy conservation",
          strength: "Maintaining positive outlook during challenges",
          actionStep: "Practice 5-minute mindfulness sessions daily"
        }
      };
    }
  }
}

export default new ChatService();
